# -*- coding: utf-8 -*-
"""Wav2Vec2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NyiSR9do0WVaPEaB00sZ8s0eJ9ud2gd6
"""

from google.colab import drive
drive.mount('/content/drive')

"""Preprocessing for Wav2Vec2 (Audio Dataset)"""

# Audio Preprocessing for Wav2Vec2
# Author: Ayush Dutta


import os
import pandas as pd
import torchaudio
from tqdm import tqdm

# 1. Define dataset root
DATASET_DIR = "dataset/audio_files"  # Folder with subfolders per species
OUTPUT_CSV = "bird_audio_dataset.csv"

#  2. Collect file paths
data = []
for species in os.listdir(DATASET_DIR):
    species_dir = os.path.join(DATASET_DIR, species)
    if not os.path.isdir(species_dir):
        continue
    for file in os.listdir(species_dir):
        if file.endswith((".wav", ".mp3")):
            path = os.path.join(species_dir, file)
            data.append({"file_path": path, "label": species})

df = pd.DataFrame(data)
df.to_csv(OUTPUT_CSV, index=False)
print(f" Metadata saved to {OUTPUT_CSV} with {len(df)} entries.")

# 3. Audio sanity check (optional)
waveform, sr = torchaudio.load(df.iloc[0]['file_path'])
print(f"Sample loaded: {df.iloc[0]['file_path']} - Sample rate: {sr}, Duration: {waveform.shape[1]/sr:.2f}s")

"""Script 1 — Wav2Vec2 Fine-Tuning (Audio Classification)"""

# Wav2Vec2 Fine-Tuning for Bird Species Classification
# Author: Ayush Dutta
# Date: 2025


#  1. Library Imports
import os
import torch
import librosa
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import matplotlib.pyplot as plt

# ==== 2. Data Loading ====
# Example: Dataset structure should have columns: [file_path, label]
data = pd.read_csv("bird_audio_dataset.csv")  # replace with your CSV path
print("Dataset loaded:", data.shape)

#  3. Preprocessing
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")

def load_audio(batch):
    audio, sr = librosa.load(batch["file_path"], sr=16000)
    batch["input_values"] = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True).input_values[0]
    batch["labels"] = batch["label"]
    return batch

dataset = Dataset.from_pandas(data)
dataset = dataset.map(load_audio)

train_test = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test["train"]
test_dataset = train_test["test"]

#  4. Model Setup
model = Wav2Vec2ForSequenceClassification.from_pretrained(
    "facebook/wav2vec2-base",
    num_labels=len(set(data["label"])),
    problem_type="single_label_classification"
)

#  5. Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-4,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

def compute_metrics(pred):
    preds = np.argmax(pred.predictions, axis=1)
    acc = accuracy_score(pred.label_ids, preds)
    f1 = f1_score(pred.label_ids, preds, average="macro")
    return {"accuracy": acc, "macro_f1": f1}

#  6. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=processor,
    compute_metrics=compute_metrics
)

trainer.train()

#  7. Evaluation
predictions = trainer.predict(test_dataset)
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=1)

print("\n=== Wav2Vec2 Evaluation Results ===")
print(classification_report(y_true, y_pred))
print("Accuracy:", accuracy_score(y_true, y_pred))
print("Macro-F1:", f1_score(y_true, y_pred, average="macro"))

#  8. Save Model
trainer.save_model("wav2vec2_bird_model")
processor.save_pretrained("wav2vec2_processor")
print(" Model and processor saved.")

"""Preprocessing for AutoML CNN (Spectrogram Generation)"""

# Spectrogram Preprocessing for AutoML CNN
# Converts audio files (mp3/wav) → 3-channel spectrogram images

# Author: Ayush Dutta

import os
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# 1. Define paths
AUDIO_ROOT = "dataset/audio_files"        # Input audio directory
OUTPUT_ROOT = "dataset/audio_spectrograms"  # Output image directory
os.makedirs(OUTPUT_ROOT, exist_ok=True)

#  2. Function to generate spectrogram
def save_spectrogram(audio_path, output_path, sr=22050):
    try:
        y, sr = librosa.load(audio_path, sr=sr)
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
        S_DB = librosa.power_to_db(S, ref=np.max)

        # Convert to 3-channel RGB image
        fig = plt.figure(figsize=(3, 3))
        plt.axis('off')
        librosa.display.specshow(S_DB, sr=sr, x_axis=None, y_axis=None, cmap='magma')
        plt.tight_layout(pad=0)
        plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
        plt.close(fig)
    except Exception as e:
        print(f" Error processing {audio_path}: {e}")

# 3. Convert all audio files
for species in os.listdir(AUDIO_ROOT):
    input_dir = os.path.join(AUDIO_ROOT, species)
    output_dir = os.path.join(OUTPUT_ROOT, species)
    os.makedirs(output_dir, exist_ok=True)

    for file in tqdm(os.listdir(input_dir), desc=f"Processing {species}"):
        if file.endswith((".wav", ".mp3")):
            input_path = os.path.join(input_dir, file)
            output_path = os.path.join(output_dir, file.replace('.mp3', '.png').replace('.wav', '.png'))
            save_spectrogram(input_path, output_path)

print(f" All spectrograms saved in {OUTPUT_ROOT}")

